<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Richard Tang - Projects</title>

    <script src="https://cdn.tailwindcss.com"></script>
    <script>
      tailwind.config = {
        theme: {
          fontFamily: {
            sans: ["Ubuntu", "sans-serif"],
          },
        },
      };
    </script>

    <link
      href="https://fonts.googleapis.com/css2?family=Ubuntu&display=swap"
      rel="stylesheet"
    />

    <style>
      /* Custom styles for modal */
      .modal {
        backdrop-filter: blur(8px);
        animation: fadeIn 0.2s ease-out;
        position: fixed !important;
        top: 0 !important;
        left: 0 !important;
        width: 100vw !important;
        height: 100vh !important;
        margin: 0 !important;
        padding: 0 !important;
        z-index: 9999 !important;
      }
      
      .modal-content {
        animation: scaleIn 0.2s ease-out;
        max-width: 90vw;
        max-height: 90vh;
      }
      
      @keyframes fadeIn {
        from { opacity: 0; }
        to { opacity: 1; }
      }
      
      @keyframes scaleIn {
        from { transform: scale(0.9); opacity: 0; }
        to { transform: scale(1); opacity: 1; }
      }

      /* Consistent thumbnail styling */
      .project-thumbnail {
        width: 100%;
        height: 12rem; /* Consistent height */
        object-fit: cover;
        object-position: center;
        cursor: pointer;
        transition: transform 0.2s ease, filter 0.2s ease;
        border-radius: 0.5rem;
      }

      .project-thumbnail:hover {
        transform: scale(1.02);
        filter: brightness(1.1);
      }

      /* Click indicator */
      .thumbnail-overlay {
        position: absolute;
        top: 0.5rem;
        right: 0.5rem;
        background: rgba(0, 0, 0, 0.7);
        color: white;
        padding: 0.25rem 0.5rem;
        border-radius: 0.25rem;
        font-size: 0.75rem;
        opacity: 0;
        transition: opacity 0.2s ease;
      }

      .project-card:hover .thumbnail-overlay {
        opacity: 1;
      }

      /* Loading animations */
      .loading-spinner {
        width: 40px;
        height: 40px;
        border: 4px solid rgba(255, 255, 255, 0.3);
        border-radius: 50%;
        border-top-color: #fff;
        animation: spin 1s ease-in-out infinite;
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
      }

      @keyframes spin {
        to { transform: translate(-50%, -50%) rotate(360deg); }
      }

      /* Page transitions */
      .page-transition {
        animation: fadeInUp 0.8s ease-out;
      }

      @keyframes fadeInUp {
        from {
          opacity: 0;
          transform: translateY(30px);
        }
        to {
          opacity: 1;
          transform: translateY(0);
        }
      }

      /* Smooth scroll behavior */
      html {
        scroll-behavior: smooth;
      }

      /* Enhanced hover effects */
      .project-thumbnail {
        transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
      }

      .project-thumbnail:hover {
        transform: scale(1.03);
        filter: brightness(1.1);
        box-shadow: 0 10px 25px rgba(0, 0, 0, 0.15);
      }

      /* Staggered animations for project cards */
      .project-card:nth-child(1) { animation-delay: 0.1s; }
      .project-card:nth-child(2) { animation-delay: 0.2s; }
      .project-card:nth-child(3) { animation-delay: 0.3s; }
      .project-card:nth-child(4) { animation-delay: 0.4s; }
      .project-card:nth-child(5) { animation-delay: 0.5s; }
      .project-card:nth-child(6) { animation-delay: 0.6s; }
      .project-card:nth-child(7) { animation-delay: 0.7s; }
      .project-card:nth-child(8) { animation-delay: 0.8s; }
      .project-card:nth-child(9) { animation-delay: 0.9s; }
      .project-card:nth-child(10) { animation-delay: 1.0s; }
      .project-card:nth-child(11) { animation-delay: 1.1s; }
      .project-card:nth-child(12) { animation-delay: 1.2s; }
      .project-card:nth-child(13) { animation-delay: 1.3s; }
      .project-card:nth-child(14) { animation-delay: 1.4s; }
      .project-card:nth-child(15) { animation-delay: 1.5s; }
      .project-card:nth-child(16) { animation-delay: 1.6s; }

      /* Filter transition animations */
      .project-card {
        transition: all 0.3s ease;
      }

      .project-card.fade-out {
        opacity: 0;
        transform: scale(0.9);
      }

      .project-card.fade-in {
        opacity: 1;
        transform: scale(1);
      }
    </style>
  </head>
  <body class="bg-[#e3dcd2]">
    <nav
      class="bg-red-800 text-white text-3xl font-bold px-6 py-4 flex items-center flex-initial justify-between"
    >
      <div class="hover:text-[#e3dcd2]">
        <a href="index.html">RICHARD TANG</a>
      </div>

      <div class="flex space-x-6">
        <a href="index.html#about" class="hover:text-[#e3dcd2] px-3 py-2"
          >About Me</a
        >
        <a href="projects.html" class="hover:text-[#e3dcd2] px-3 py-2">
          Projects</a
        >
        <a href="resources/resume.pdf" class="hover:text-[#e3dcd2] px-3 py-2"> Resume</a>
      </div>
    </nav>

    <div class="max-w-4xl mx-auto mt-20 px-6 space-y-12 page-transition">
      <h1 class="text-4xl font-bold text-center">My Projects</h1>

      <!-- Filter Buttons -->
      <div class="flex justify-center space-x-4 mb-8">
        <button
          class="filter-btn bg-red-800 text-white px-4 py-2 rounded-lg"
          data-filter="all"
        >
          All
        </button>
        <button
          class="filter-btn bg-yellow-200 text-yellow-800 px-4 py-2 rounded-lg"
          data-filter="Reinforcement Learning"
        >
          Reinforcement Learning
        </button>
        <button
          class="filter-btn bg-blue-200 text-blue-800 px-4 py-2 rounded-lg"
          data-filter="Computer Vision"
        >
          Computer Vision
        </button>
        <button
          class="filter-btn bg-gray-200 text-gray-800 px-4 py-2 rounded-lg"
          data-filter="Misc"
        >
          Misc
        </button>
      </div>

      <div id="projects-container">
        <!-- Project 1 -->
        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Reinforcement Learning">
          <div class="relative mb-6">
            <img src="assets/goal_rl.gif" alt="Goal Conditioned Framework" class="project-thumbnail" onclick="openModal('assets/goal_rl.gif', 'Goal Conditioned Framework')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/Goal-Conditioned-RL-Framework" target="_blank" class="text-3xl font-bold hover:underline">
                Goal Conditioned Framework
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-yellow-200 text-yellow-800 text-sm font-medium px-2.5 py-0.5 rounded">Reinforcement Learning</span>
            </div>
            <p class="text-lg text-gray-700">
              This repository implements a goal-conditioned reinforcement learning framework with DDPG, SAC, TD3 and TQC for the Panda-Gym environment on OpenAI gymnasium. The framework trains the Franka-Emika Arm to accomplish reaching, pushing, sliding and pick-place tasks to high success rates.
            </p>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Computer Vision">
          <div class="relative mb-6">
            <img src="assets/amstra.gif" alt="AMSTRA" class="project-thumbnail" onclick="openModal('assets/amstra.gif', 'AMSTRA')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/AMSTRA" target="_blank" class="text-3xl font-bold hover:underline">
                AMSTRA
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-blue-200 text-blue-800 text-sm font-medium px-2.5 py-0.5 rounded">Computer Vision</span>
            </div>
            <p class="text-lg text-gray-700">
              AMSTRA is a designed framework for autonomous and manually controlled drone navigation with real-time object detection and spatial tagging modules. It leverages YoloV8 and SORT-based manager to identify, and track objects of interest. A separate triangulation module uses monocular camera stream to attempt 3D spatial tagging of detected objects. The system is designed for real-time operation, featuring a server-client architecture to balance computation load.
            </p>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Computer Vision">
          <div class="relative mb-6">
            <img src="assets/genitext.gif" alt="GenIText" class="project-thumbnail" onclick="openModal('assets/genitext.gif', 'GenIText')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/GenIText" target="_blank" class="text-3xl font-bold hover:underline">
                GenIText
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-blue-200 text-blue-800 text-sm font-medium px-2.5 py-0.5 rounded">Computer Vision</span>
            </div>
            <p class="text-lg text-gray-700">
              This repository is independently developed as a flexible framework to generate high-quality Image-Text pairs for finetuning Image-Generation models, such as Stable Diffusion, DALL-E, and other generative models. It leverages open-source VLLMs to caption unlabeled image data for various output format to use in stable-diffusion models.
            </p>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Reinforcement Learning">
          <div class="relative mb-6">
            <img src="assets/mujrl.gif" alt="MujRL" class="project-thumbnail" onclick="openModal('assets/mujrl.gif', 'MujRL')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/MujRL" target="_blank" class="text-3xl font-bold hover:underline">
                MujRL
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-yellow-200 text-yellow-800 text-sm font-medium px-2.5 py-0.5 rounded">Reinforcement Learning</span>
            </div>
            <p class="text-lg text-gray-700">
              This project provides implementations of two state-of-the-art reinforcement learning algorithms: Twin Delayed Deep Deterministic Policy Gradient (TD3) and Soft Actor-Critic (SAC). Both algorithms are implemented for various MuJoCo environments, including Ant, HalfCheetah, Hopper, Inverted Pendulum, Inverted Double Pendulum, Walker, and reacher. Resulting models are able to perform manipulation or movement tasks competently
            </p>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Reinforcement Learning">
          <div class="relative mb-6">
            <img src="assets/space_invaders_egm.gif" alt="Space Invaders DQN" class="project-thumbnail" onclick="openModal('assets/space_invaders_egm.gif', 'Towards Efficient Deep Q-Learning for Space Invaders')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/Towards-Efficient-Deep-Q-Learning-for-Space-Invaders-" target="_blank" class="text-3xl font-bold hover:underline">
                Towards Efficient Deep Q-Learning for Space Invaders
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-yellow-200 text-yellow-800 text-sm font-medium px-2.5 py-0.5 rounded">Reinforcement Learning</span>
            </div>
            <p class="text-lg text-gray-700">
              This repository is a resource-conscious reinforcement learning framework designed to train Deep Q-Network (DQN) agents for Atari games—specifically Space Invaders—on consumer-grade hardware. This project explores architectural optimizations and training strategies that reduce compute and memory restrictions while retaining favorable performance. Overall, the optimized DQN model outperform standard DQN models by 57% while scaling more efficiently.
            </p>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Computer Vision">
          <div class="relative mb-6">
            <img src="assets/cnn_posenet.gif" alt="CNN PoseNet Suite" class="project-thumbnail" onclick="openModal('assets/cnn_posenet.gif', 'CNN PoseNet Suite')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/CNN-PoseNet-Suite" target="_blank" class="text-3xl font-bold hover:underline">
                CNN PoseNet Suite
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-blue-200 text-blue-800 text-sm font-medium px-2.5 py-0.5 rounded">Computer Vision</span>
            </div>
            <p class="text-lg text-gray-700">
              This repository focuses on 6-DoF camera pose estimation using various deep learning models on the 7-Scenes dataset. Specifically, the repository expands on the original paper to train Posenets with different CNN backbones for performance tradeoff comparison. While `ResNet`-based PoseNets offered faster inference time, performance overall for PoseNets were limited, as seen from visualized trajectories.
            </p>
          </div>
        </div>
        
        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Computer Vision">
          <div class="relative mb-6">
            <img src="assets/unet_seg.png" alt="UNet Segmentation" class="project-thumbnail" onclick="openModal('assets/unet_seg.png', 'UNet Image Segmentation for Aerial Imagery of Dubai')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/UNet-Image-Segmentation" target="_blank" class="text-3xl font-bold hover:underline">
                UNet Image Segmentation for Aerial Imagery of Dubai
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-blue-200 text-blue-800 text-sm font-medium px-2.5 py-0.5 rounded">Computer Vision</span>
            </div>
            <p class="text-lg text-gray-700">
              This repository contains the implementation of UNet for Semantic Segmentation of Aerial Images, classifying land, water, buildings, roads, and vegetation using aerial images over Dubai. While underperforming on small or underepresented features, the UNet model performed competently on larger image features such as land, roads, and buildings.
            </p>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Computer Vision">
          <div class="relative mb-6">
            <img src="assets/unet_denoise.png" alt="UNet Denoising" class="project-thumbnail" onclick="openModal('assets/unet_denoise.png', 'UNet Image Denoising for COCO Dataset')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/UNet-Image-Denoising" target="_blank" class="text-3xl font-bold hover:underline">
                UNet Image Denoising for COCO Dataset
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-blue-200 text-blue-800 text-sm font-medium px-2.5 py-0.5 rounded">Computer Vision</span>
            </div>
            <p class="text-lg text-gray-700">
              This is an educational repository for training a standard U-Net model for image denoising. The project supports training with either a consistent noise level or varying noise levels across different data samples. The UNet model performed general image denoising to sufficient degree but shows room for improvement on finer detail recovery, possibly due to needing to learn from multiple noise levels simultaneously.
            </p>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Computer Vision">
          <div class="relative mb-6">
            <img src="assets/nerf.gif" alt="NeRF Implementation" class="project-thumbnail" onclick="openModal('assets/nerf.gif', 'Neural Radiance Field Implementation')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/NeRF-Pytorch-Implementation" target="_blank" class="text-3xl font-bold hover:underline">
                Neural Radiance Field Implementation
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-blue-200 text-blue-800 text-sm font-medium px-2.5 py-0.5 rounded">Computer Vision</span>
            </div>
            <p class="text-lg text-gray-700">
              This is an educational repository on the implementation of NeRF for novel view synthesis, rendering seen objects at unseen angles. Some minor extension work involved using mask losses for foreground and background to improve rendering performance.
            </p>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Computer Vision">
          <div class="relative mb-6">
            <img src="assets/esrgan.gif" alt="ESRGAN" class="project-thumbnail" onclick="openModal('assets/esrgan.gif', 'Enhanced Super-Resolution Generative Adversarial Network')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/ESRGAN-pytorch" target="_blank" class="text-3xl font-bold hover:underline">
                Enhanced Super-Resolution Generative Adversarial Network
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-blue-200 text-blue-800 text-sm font-medium px-2.5 py-0.5 rounded">Computer Vision</span>
            </div>
            <p class="text-lg text-gray-700">
              This repository implements ESRGAN (Enhanced Super-Resolution Generative Adversarial Network) for image super-resolution tasks to high fidelity. The model is trained on a small set of high definition images and is generalizable to a wide variety of single-image contexts.
            </p>
          </div>
        </div>
        
        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Misc">
          <div class="relative mb-6">
            <img src="assets/cognitus.png" alt="Cognitus Chess Engine" class="project-thumbnail" onclick="openModal('assets/cognitus.png', 'Cognitus: Python-based Chess Engine with Alpha-beta Pruning')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/Chess-Engine-Cognitus" target="_blank" class="text-3xl font-bold hover:underline">
                Cognitus: Python-based Chess Engine with Alpha-beta Pruning
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-gray-200 text-gray-800 text-sm font-medium px-2.5 py-0.5 rounded">Misc</span>
            </div>
            <p class="text-lg text-gray-700">
              This is a python-based chess engine for deploying algorithmic-based min-maxing game-playing agents with Alpha-Beta pruning. Current ELO rating is estimated between 1550 elo and 1750 elo, performing at evaluations with 4-7 depths due to computation constraints.
            </p>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Computer Vision">
          <div class="relative mb-6">
            <img src="assets/canny_edge.gif" alt="Canny Edge Detection" class="project-thumbnail" onclick="openModal('assets/canny_edge.gif', 'Canny-Edge Detection')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/ESRGAN-pytorch" target="_blank" class="text-3xl font-bold hover:underline">
                Canny-Edge Detection
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-blue-200 text-blue-800 text-sm font-medium px-2.5 py-0.5 rounded">Computer Vision</span>
            </div>
            <p class="text-lg text-gray-700">
              This project demonstrates the full Canny Edge Detection algorithm, highlighting its core logic and enhancements for accuracy, robustness, and speed. It includes image and video inference scripts, with optional FFT-based convolution for faster Gaussian filtering. The repository is built as an educational resource, making the algorithm's inner workings accessible while offering configurable parameters for experimentation.
            </p>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Reinforcement Learning">
          <div class="relative mb-6">
            <img src="assets/traffic_rl.gif" alt="Traffic Control RL" class="project-thumbnail" onclick="openModal('assets/traffic_rl.gif', 'Traffic Control with Reinforcement Learning')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/Traffic-Control-RL" target="_blank" class="text-3xl font-bold hover:underline">
                Traffic Control with Reinforcement Learning
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-yellow-200 text-yellow-800 text-sm font-medium px-2.5 py-0.5 rounded">Reinforcement Learning</span>
            </div>
            <p class="text-lg text-gray-700">
              This project applies deep reinforcement learning to optimize traffic signal timing using SUMO simulations. A modified DuelDQN model enables weight transfer from single-intersection tasks, allowing agents to quickly adapt to multi-intersection n×n traffic control scenarios. By leveraging shared base policies, multiple agents can efficiently coordinate and improve traffic flow under unseen conditions.
            </p>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Reinforcement Learning">
          <div class="relative mb-6">
            <img src="assets/c_lander_video.gif" alt="Lunar Lander" class="project-thumbnail" onclick="openModal('assets/c_lander_video.gif', 'Lunar Lander with varying action space using Actor-Critic model')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/Traffic-Control-RL" target="_blank" class="text-3xl font-bold hover:underline">
                Lunar Lander with varying action space using Actor-Critic model
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-yellow-200 text-yellow-800 text-sm font-medium px-2.5 py-0.5 rounded">Reinforcement Learning</span>
            </div>
            <p class="text-lg text-gray-700">
              This project explores reinforcement learning on OpenAI Gym's LunarLander-v2 using both Deep Q-Networks (for discrete control) and Actor-Critic methods (for continuous control). The discrete models are lightweight feedforward networks, while the continuous Actor-Critic setup leverages Gaussian policy outputs, a Q-value Critic, and action noise for exploration. Results show that Actor-Critic converges up to 4× faster than DQNs, highlighting its efficiency in continuous action spaces.
            </p>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Computer Vision">
          <div class="relative mb-6">
            <img src="assets/stable_diffusion.gif" alt="Stable Diffusion" class="project-thumbnail" onclick="openModal('assets/stable_diffusion.gif', 'Custom Fine-tuning Framework for Stable-Diffusion')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/Stable-Diffusion-Pytorch" target="_blank" class="text-3xl font-bold hover:underline">
                Custom Fine-tuning Framework for prompt-based Image Generation using Stable-Diffusion with Pytorch
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-blue-200 text-blue-800 text-sm font-medium px-2.5 py-0.5 rounded">Computer Vision</span>
            </div>
            <p class="text-lg text-gray-700">
              This project provides an educational framework for fine-tuning the Stable Diffusion model via HuggingFace, supporting both conditional and unconditional image training. Instead of re-implementing Stable Diffusion from scratch, it offers a lightweight, configurable class for efficient fine-tuning tasks. Future extensions aim to cover image-to-image translation, inpainting, and related applications.
            </p>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-6 page-transition" data-tags="Misc">
          <div class="relative mb-6">
            <img src="assets/audio_visual.gif" alt="FFT Audio Visualization" class="project-thumbnail" onclick="openModal('assets/audio_visual.gif', 'Fast Fourier Transformation for Audio Visualization')">
            <div class="thumbnail-overlay">Click to expand</div>
          </div>
          <div class="space-y-4">
            <div class="flex justify-between items-center">
              <a href="https://github.com/CodeKnight314/FFT-Audio-Visualization" target="_blank" class="text-3xl font-bold hover:underline">
                Fast Fourier Transformation for Audio Visualization
              </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
            </div>
            <div class="flex flex-wrap gap-2">
              <span class="bg-gray-200 text-gray-800 text-sm font-medium px-2.5 py-0.5 rounded">Misc</span>
            </div>
            <p class="text-lg text-gray-700">
              This project demonstrates the use of Fast Fourier Transform (FFT) to decompose audio signals into their frequency components. It includes a rendering pipeline (render.py) that converts MP3 audio into frame-by-frame visualizations of normalized magnitude vs. frequency, compiled into an MP4 video matching the audio duration. A sample 60 fps rendered video is provided for reference.
            </p>
          </div>
        </div>
    </div>

    <!-- Modal -->
    <div id="imageModal" class="modal fixed inset-0 bg-black bg-opacity-75 flex items-center justify-center z-50 hidden" onclick="closeModal(event)">
      <div class="modal-content relative">
        <button onclick="closeModal()" class="absolute -top-10 -right-10 text-white text-4xl hover:text-gray-300 z-10">&times;</button>
        <img id="modalImage" src="" alt="" class="max-w-full max-h-full rounded-lg shadow-2xl">
        <div id="modalCaption" class="text-white text-center mt-4 text-lg font-medium"></div>
      </div>
    </div>

    <script>
      // Modal functionality with loading spinner
      function openModal(imageSrc, caption) {
        const modal = document.getElementById('imageModal');
        const modalImage = document.getElementById('modalImage');
        const modalCaption = document.getElementById('modalCaption');
        const modalContent = modal.querySelector('.modal-content');
        
        // Remove any existing spinner
        const existingSpinner = modal.querySelector('.loading-spinner');
        if (existingSpinner) {
          existingSpinner.remove();
        }
        
        // Create and show loading spinner
        const spinner = document.createElement('div');
        spinner.className = 'loading-spinner';
        modalContent.appendChild(spinner);
        
        // Hide image initially
        modalImage.style.opacity = '0';
        modalImage.style.display = 'none';
        
        // Show modal
        modal.classList.remove('hidden');
        document.body.style.overflow = 'hidden';
        modalCaption.textContent = caption;
        
        // Preload image
        const img = new Image();
        img.onload = function() {
          // Remove spinner
          spinner.remove();
          
          // Set image and show with animation
          modalImage.src = imageSrc;
          modalImage.alt = caption;
          modalImage.style.display = 'block';
          modalImage.style.opacity = '1';
          modalImage.style.animation = 'scaleIn 0.3s ease-out';
        };
        
        img.onerror = function() {
          // Remove spinner and show error
          spinner.remove();
          modalCaption.textContent = 'Error loading image: ' + caption;
        };
        
        img.src = imageSrc;
      }

      function closeModal(event) {
        // Don't close if clicking on the image itself
        if (event && event.target.id === 'modalImage') {
          return;
        }
        
        const modal = document.getElementById('imageModal');
        modal.classList.add('hidden');
        document.body.style.overflow = 'auto'; // Restore scrolling
      }

      // Close modal with Escape key
      document.addEventListener('keydown', function(event) {
        if (event.key === 'Escape') {
          closeModal();
        }
      });

      // Filter functionality
      document.addEventListener("DOMContentLoaded", () => {
        const filterButtons = document.querySelectorAll(".filter-btn");
        const projectCards = document.querySelectorAll(".project-card");

        const colorConfig = {
          "Reinforcement Learning": ["bg-yellow-200", "text-yellow-800"],
          "Computer Vision": ["bg-blue-200", "text-blue-800"],
          Misc: ["bg-gray-200", "text-gray-800"],
          all: ["bg-gray-200", "text-gray-800"],
        };

        const selectedColor = ["bg-red-800", "text-white"];

        function resetButtonColors() {
          filterButtons.forEach((btn) => {
            const filter = btn.dataset.filter;
            btn.classList.remove(...selectedColor);
            if (colorConfig[filter]) {
              btn.classList.add(...colorConfig[filter]);
            }
          });
        }

        filterButtons.forEach((button) => {
          button.addEventListener("click", () => {
            const filter = button.dataset.filter;

            // Update button styles
            resetButtonColors();
            button.classList.remove(...(colorConfig[filter] || []));
            button.classList.add(...selectedColor);

            // Filter project cards with smooth animations
            projectCards.forEach((card, index) => {
              const tags = card.dataset.tags.split(",");
              const shouldShow = filter === "all" || tags.includes(filter);
              
              if (shouldShow) {
                // Show card with staggered animation
                card.classList.remove('fade-out');
                card.classList.add('fade-in');
                card.style.display = "block";
                card.style.animationDelay = `${index * 0.1}s`;
                
                // Re-trigger the page transition animation
                setTimeout(() => {
                  card.style.animation = 'fadeInUp 0.6s ease-out';
                }, index * 100);
              } else {
                // Hide card with fade out
                card.classList.remove('fade-in');
                card.classList.add('fade-out');
                
                setTimeout(() => {
                  if (card.classList.contains('fade-out')) {
                    card.style.display = "none";
                  }
                }, 300);
              }
            });
          });
        });

        // Set initial state
        const initialFilter = document.querySelector(
          '.filter-btn[data-filter="all"]'
        );
        initialFilter.classList.remove(...colorConfig["all"]);
        initialFilter.classList.add(...selectedColor);
      });
    </script>
  </body>
</html>