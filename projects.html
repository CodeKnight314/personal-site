<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Richard Tang - Projects</title>

    <script src="https://cdn.tailwindcss.com"></script>
    <script>
      tailwind.config = {
        theme: {
          fontFamily: {
            sans: ["Ubuntu", "sans-serif"],
          },
        },
      };
    </script>

    <link
      href="https://fonts.googleapis.com/css2?family=Ubuntu&display=swap"
      rel="stylesheet"
    />
  </head>
  <body class="bg-[#e3dcd2]">
    <nav
      class="bg-red-800 text-white text-3xl font-bold px-6 py-4 flex items-center flex-initial justify-between"
    >
      <div class="hover:text-[#e3dcd2]">
        <a href="index.html">RICHARD TANG</a>
      </div>

      <div class="flex space-x-6">
        <a href="index.html#about" class="hover:text-[#e3dcd2] px-3 py-2"
          >About Me</a
        >
        <a href="projects.html" class="hover:text-[#e3dcd2] px-3 py-2">
          Projects</a
        >
        <a href="" class="hover:text-[#e3dcd2] px-3 py-2"> Resume</a>
      </div>
    </nav>

    <div class="max-w-4xl mx-auto mt-20 px-6 space-y-12">
      <h1 class="text-4xl font-bold text-center">My Projects</h1>

      <!-- Filter Buttons -->
      <div class="flex justify-center space-x-4 mb-8">
        <button
          class="filter-btn bg-red-800 text-white px-4 py-2 rounded-lg"
          data-filter="all"
        >
          All
        </button>
        <button
          class="filter-btn bg-yellow-200 text-yellow-800 px-4 py-2 rounded-lg"
          data-filter="Reinforcement Learning"
        >
          Reinforcement Learning
        </button>
        <button
          class="filter-btn bg-blue-200 text-blue-800 px-4 py-2 rounded-lg"
          data-filter="Computer Vision"
        >
          Computer Vision
        </button>
        <button
          class="filter-btn bg-gray-200 text-gray-800 px-4 py-2 rounded-lg"
          data-filter="Misc"
        >
          Misc
        </button>
      </div>

      <div id="projects-container">
        <!-- Project 1 -->
        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Reinforcement Learning">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/goal_rl.gif" alt="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/Goal-Conditioned-RL-Framework" target="_blank" class="text-3xl font-bold hover:underline">
                  Goal Conditioned Framework
                </a>
                <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-yellow-200 text-yellow-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Reinforcement Learning</span>
              </div>
              <p class="text-lg text-gray-700">
                This repository implements a goal-conditioned reinforcement learning framework with DDPG, SAC, TD3 and TQC for the Panda-Gym environment on OpenAI gymnasium. The framework trains the Franka-Emika Arm to accomplish reaching, pushing, sliding and pick-place tasks to high success rates.
              </p>
            </div>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Computer Vision">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/amstra.gif" atl="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/AMSTRA" target="_blank" class="text-3xl font-bold hover:underline">
                  AMSTRA 
                </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-blue-200 text-blue-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Computer Vision</span>
              </div>
              <p class="text-lg text-gray-700">
                AMSTRA is a designed framework for autonomous and manually controlled drone navigation with real-time object detection and spatial tagging modules. It leverages YoloV8 and SORT-based manager to identify, and track objects of interest. A separate triangulation module uses monocular camera stream to attempt 3D spatial tagging of detected objects. The system is designed for real-time operation, featuring a server-client architecture to balance computation load.
              </p>
            </div>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Computer Vision">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/genitext.gif" atl="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/GenIText" target="_blank" class="text-3xl font-bold hover:underline">
                  GenIText 
                </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-blue-200 text-blue-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Computer Vision</span>
              </div>
              <p class="text-lg text-gray-700">
                This repository is independently developed as a felxible framework to generate high-quality Image-Text pairs for finetuning Image-Generation models, such as Stable Diffusion, DALL-E, and other generative models. It leverages open-source VLLMs to caption unlabeled image data for various output format to use in stable-diffusion models.
              </p>
            </div>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Reinforcement Learning">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/mujrl.gif" alt="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/MujRL" target="_blank" class="text-3xl font-bold hover:underline">
                  MujRL
                </a>
                <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-yellow-200 text-yellow-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Reinforcement Learning</span>
              </div>
              <p class="text-lg text-gray-700">
                This project provides implementations of two state-of-the-art reinforcement learning algorithms: Twin Delayed Deep Deterministic Policy Gradient (TD3) and Soft Actor-Critic (SAC). Both algorithms are implemented for various MuJoCo environments, including Ant, HalfCheetah, Hopper, Inverted Pendulum, Inverted Double Pendulum, Walker, and reacher. Resulting models are able to perform manipulation or movement tasks competently 
              </p>
            </div>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Reinforcement Learning">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/space_invaders_egm.gif" alt="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/Towards-Efficient-Deep-Q-Learning-for-Space-Invaders-" target="_blank" class="text-3xl font-bold hover:underline">
                  Towards Efficient Deep Q-Learning for Space Invaders
                </a>
                <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-yellow-200 text-yellow-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Reinforcement Learning</span>
              </div>
              <p class="text-lg text-gray-700">
                This repository is a resource-conscious reinforcement learning framework designed to train Deep Q-Network (DQN) agents for Atari games—specifically Space Invaders—on consumer-grade hardware. This project explores architectural optimizations and training strategies that reduce compute and memory restrictions while retaining favorable performance. Overall, the optimized DQN model outperform standard DQN models by 57% while scaling more efficiently.              
              </p>
            </div>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Computer Vision">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/cnn_posenet.gif" atl="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/CNN-PoseNet-Suite" target="_blank" class="text-3xl font-bold hover:underline">
                  CNN PoseNet Suite 
                </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-blue-200 text-blue-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Computer Vision</span>
              </div>
              <p class="text-lg text-gray-700">
                This repository focuses on 6-DoF camera pose estimation using various deep learning models on the 7-Scenes dataset. Specifically, the repository expands on the original paper to train Posenets with different CNN backbones for performance tradeoff comparison. While `ResNet`-based PoseNets offered faster inference time, performance overall for PoseNets were limited, as seen from visualized trajectories.
              </p>
            </div>
          </div>
        </div>
        
        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Computer Vision">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/unet_seg.png" atl="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/UNet-Image-Segmentation" target="_blank" class="text-3xl font-bold hover:underline">
                  UNet Image Segmentation for Aerial Imagery of Dubai
                </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-blue-200 text-blue-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Computer Vision</span>
              </div>
              <p class="text-lg text-gray-700">
                This repository contains the implementation of UNet for Semantic Segmentation of Aerial Images, classifying land, water, buildings, roads, and vegetation using aerial images over Dubai. While underperforming on small or underepresented features, the UNet model performed competently on larger image features such as land, roads, and buildings.
              </p>
            </div>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Computer Vision">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/unet_denoise.png" atl="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/UNet-Image-Denoising" target="_blank" class="text-3xl font-bold hover:underline">
                  UNet Image Denoising for COCO Dataset
                </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-blue-200 text-blue-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Computer Vision</span>
              </div>
              <p class="text-lg text-gray-700">
                This is an educational repository for training a standard U-Net model for image denoising. The project supports training with either a consistent noise level or varying noise levels across different data samples. The UNet model performed general image denoising to sufficient degree but shows room for improvement on finer detail recovery, possibly due to needing to learn from multiple noise levels simultaneously.
              </p>
            </div>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Computer Vision">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/nerf.gif" atl="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/NeRF-Pytorch-Implementation" target="_blank" class="text-3xl font-bold hover:underline">
                  Neural Radiance Field Implementation
                </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-blue-200 text-blue-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Computer Vision</span>
              </div>
              <p class="text-lg text-gray-700">
                This is an educational repository on the implementation of NeRF for novel view synthesis, rendering seen objects at unseen angles. Some minor extension work involved using mask losses for foreground and background to improve rendering performance.
              </p>
            </div>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Computer Vision">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/esrgan.gif" atl="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/ESRGAN-pytorch" target="_blank" class="text-3xl font-bold hover:underline">
                  Enhanced Super-Resolution Generative Adversarial Network
                </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-blue-200 text-blue-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Computer Vision</span>
              </div>
              <p class="text-lg text-gray-700">
                This repository implements ESRGAN (Enhanced Super-Resolution Generative Adversarial Network) for image super-resolution tasks to high fidelity. The model is trained on a small set of high definition images and is generalizable to a wide variety of single-image contexts.
              </p>
            </div>
          </div>
        </div>
        
        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Misc">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/cognitus.png" atl="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/ESRGAN-pytorch" target="_blank" class="text-3xl font-bold hover:underline">
                  Cognitus: Python-based Chess Engine with Alpha-beta Pruning
                </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-gray-200 text-gray-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Misc</span>
              </div>
              <p class="text-lg text-gray-700">
                This is a python-based chess engine for deploying algorithmic-based min-maxing game-playing agents with Alpha-Beta pruning. Current ELO rating is estimated between 1550 elo and 1750 elo, performing at evaluations with 4-7 depths due to computation constraints.
              </p>
            </div>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Computer Vision">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/canny_edge.gif" atl="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/ESRGAN-pytorch" target="_blank" class="text-3xl font-bold hover:underline">
                  Canny-Edge Detection
                </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-blue-200 text-blue-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Computer Vision</span>
              </div>
              <p class="text-lg text-gray-700">
                This project demonstrates the full Canny Edge Detection algorithm, highlighting its core logic and enhancements for accuracy, robustness, and speed. It includes image and video inference scripts, with optional FFT-based convolution for faster Gaussian filtering. The repository is built as an educational resource, making the algorithm’s inner workings accessible while offering configurable parameters for experimentation.              
              </p>
            </div>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Reinforcement Learning">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/traffic_rl.gif" alt="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/Traffic-Control-RL" target="_blank" class="text-3xl font-bold hover:underline">
                  Traffic Control with Reinforcement Learning
                </a>
                <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-yellow-200 text-yellow-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Reinforcement Learning</span>
              </div>
              <p class="text-lg text-gray-700">
                This project applies deep reinforcement learning to optimize traffic signal timing using SUMO simulations. A modified DuelDQN model enables weight transfer from single-intersection tasks, allowing agents to quickly adapt to multi-intersection n×n traffic control scenarios. By leveraging shared base policies, multiple agents can efficiently coordinate and improve traffic flow under unseen conditions.
              </p>
            </div>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Reinforcement Learning">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/c_lander_video.gif" alt="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/Traffic-Control-RL" target="_blank" class="text-3xl font-bold hover:underline">
                  Lunar Lander with varying action space using Actor-Critic model
                </a>
                <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-yellow-200 text-yellow-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Reinforcement Learning</span>
              </div>
              <p class="text-lg text-gray-700">
                This project explores reinforcement learning on OpenAI Gym’s LunarLander-v2 using both Deep Q-Networks (for discrete control) and Actor-Critic methods (for continuous control). The discrete models are lightweight feedforward networks, while the continuous Actor-Critic setup leverages Gaussian policy outputs, a Q-value Critic, and action noise for exploration. Results show that Actor-Critic converges up to 4× faster than DQNs, highlighting its efficiency in continuous action spaces.
              </p>
            </div>
          </div>
        </div>

        <div class="project-card bg-white rounded-lg shadow-lg p-6 mb-8" data-tags="Computer Vision">
          <div class="flex flex-col md:flex-row gap-6">
            <div class="md:w-2/5">
              <img src="assets/stable_diffusion.gif" atl="Project Photo" class="rounded-lg w-full">
            </div>
            <div class="md:w-3/5 space-y-4">
              <div class="flex justify-between items-center">
                <a href="https://github.com/CodeKnight314/Stable-Diffusion-Pytorch" target="_blank" class="text-3xl font-bold hover:underline">
                  Custom Fine-tuning Framework for prompt-based Image Generation using Stable-Diffusion with Pytorch
                </a>
              <span class="status-completed text-sm font-medium text-white bg-green-500 px-3 py-1 rounded-full">Completed</span>
              </div>
              <div class="flex flex-wrap gap-2">
                <span class="bg-blue-200 text-blue-800 text-sm font-medium mr-2 px-2.5 py-0.5 rounded">Computer Vision</span>
              </div>
              <p class="text-lg text-gray-700">
                This project provides an educational framework for fine-tuning the Stable Diffusion model via HuggingFace, supporting both conditional and unconditional image training. Instead of re-implementing Stable Diffusion from scratch, it offers a lightweight, configurable class for efficient fine-tuning tasks. Future extensions aim to cover image-to-image translation, inpainting, and related applications.
              </p>
            </div>
          </div>
        </div>
        
    </div>

    <script>
      document.addEventListener("DOMContentLoaded", () => {
        const filterButtons = document.querySelectorAll(".filter-btn");
        const projectCards = document.querySelectorAll(".project-card");

        const colorConfig = {
          "Reinforcement Learning": ["bg-yellow-200", "text-yellow-800"],
          "Computer Vision": ["bg-blue-200", "text-blue-800"],
          Misc: ["bg-gray-200", "text-gray-800"],
          all: ["bg-gray-200", "text-gray-800"],
        };

        const selectedColor = ["bg-red-800", "text-white"];

        function resetButtonColors() {
          filterButtons.forEach((btn) => {
            const filter = btn.dataset.filter;
            btn.classList.remove(...selectedColor);
            if (colorConfig[filter]) {
              btn.classList.add(...colorConfig[filter]);
            }
          });
        }

        filterButtons.forEach((button) => {
          button.addEventListener("click", () => {
            const filter = button.dataset.filter;

            // Update button styles
            resetButtonColors();
            button.classList.remove(...(colorConfig[filter] || []));
            button.classList.add(...selectedColor);

            // Filter project cards
            projectCards.forEach((card) => {
              const tags = card.dataset.tags.split(",");
              if (filter === "all" || tags.includes(filter)) {
                card.style.display = "block";
              } else {
                card.style.display = "none";
              }
            });
          });
        });

        // Set initial state
        const initialFilter = document.querySelector(
          '.filter-btn[data-filter="all"]'
        );
        initialFilter.classList.remove(...colorConfig["all"]);
        initialFilter.classList.add(...selectedColor);
      });
    </script>
  </body>
</html>
